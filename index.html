<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="google-site-verification" content="Uppt4GRozy9FzDdaPFzj1CaE7DWJAJWJpJ5oT9648II" />
    <title>Chengsong Huang</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <meta name="viewport" content="width=device-width">
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1><a href="./index.html">Chengsong Huang</a></h1> 
          <img src="./figure/photo.png" alt="Logo">
        <p>Email: chengsong@wustl.edu</p>
		<p><b>Links:</b></p><p><a href="./file/CV-Chengsong.pdf">[CV]</a><a href="https://github.com/Chengsong-Huang">[Github]</a><a href="https://www.linkedin.com/in/chengsong-huang-367841288/">[Linkedin]<br>
      <a href="https://www.semanticscholar.org/author/Chengsong-Huang/31937655">[Semantic Scholar]</a><a href="https://scholar.google.com/citations?hl=en&user=A7gPbV8AAAAJ&view_op=list_works&gmla=AMpAcmR9LPxNlWJo8tsFA9-D2La2SZNAtEg9eL2ZBGWwS7nxKKURxJD-S9pmara2G_181h8N8qgE98YtOGKMK1Ws5NS7l_EjCBY2TYoMPnTqJ2QJM5-yjlvSYZsrqk_0jR-6_Q">[Google Scholar]</a></p>
		
	  <p></p>



  </header>
      <section>

      <h2 id="welcome--">Welcome :-)</h2>

<p>Hello! My name is <strong>Chengsong Huang</strong>. I am a third year CSE Ph.D. candidate at <a href="https://wustl.edu/">Washington University in St. Louis</a>,advised by Prof. <a href="https://teapot123.github.io/">Jiaxin Huang</a>. Prior to WUSTL, I was an undergraduate student at Fudan University, majoring in Software Engineering.</p>

<p>I study <strong>natural language processing</strong> and multimodal-pretraining. I'm particularly interested in approaches that help tuning (Self-Play/Improving LLMs) and using large models at lower cost.</p>

<p style="color:red;">I am looking for research internship for 2026 summer in USA! </p>

<h3 id="news">News</h3>
<p>[Jan 2026] New paper <a href="https://arxiv.org/abs/2601.03986">Benchmark^2</a> and <a href="https://arxiv.org/abs/2601.05167v1">RelayLLM</a> are released. Welcome to discussion!</p>

<p>[Dec 2025] New paper <a href="https://arxiv.org/abs/2511.15661">Visplay</a>, <a href="https://arxiv.org/abs/2512.02472">R-Few</a> and <a href="https://arxiv.org/abs/2512.17043">UniRel-R1</a> are released. Thank you for my coauthors!</p>

<p>[Sep 2025] New paper <a href="https://arxiv.org/abs/2508.19652">Vision-SR1</a> and <a href="https://arxiv.org/abs/2509.07980">Parallel-R1</a> are released. Thank you for my coauthors!</p>

<p>[Aug 2025] New paper <a href="https://arxiv.org/abs/2508.05004">R-Zero</a> is released! Very happy that R-Zero is the second paper of <a href="https://huggingface.co/papers/date/2025-08-08"></a>HuggingFace's Daily Papers!</a></p>

<h3 id="education">Education</h3>

<p>Aug 2023 - Present,<br>
Ph.D. Student in Computer Science, Washington University in St. Louis.</p>

<p>Aug 2019 - Jul 2023,<br>
B.Eng. in Software Engineering, School of Computer Science, Fudan University.</p>

<h3 id="preprints">Selected Preprints</h3>

<p>RelayLLM: Efficient Reasoning via Collaborative Decoding<br>
  <ins>Chengsong Huang</ins>, Tong Zheng, Langlin Huang, Jinyuan Li, Haolin Liu, Jiaxin Huang<br>
  <a href="https://arxiv.org/abs/2601.05167v1">[Preprint]</a><a href="https://github.com/Chengsong-Huang/RelayLLM">[code]</a></p>

<p>Benchmark^2: Systematic Evaluation of LLM Benchmarks<br>
  Qi Qian*, <ins>Chengsong Huang*</ins>, Jingwen Xu, Changze Lv, Muling Wu, Wenhao Liu, Xiaohua Wang, Zhenghua Wang, Zisu Huang, Muzhao Tian, Jianhan Xu, Kun Hu, He-Da Wang, Yao Hu, Xuanjing Huang, Xiaoqing Zheng<br>
  <a href="https://arxiv.org/abs/2601.03986">[Preprint]</a></p>

<p>R-Zero: Self-Evolving Reasoning LLM from Zero Data<br>
  <ins>Chengsong Huang</ins>, Wenhao Yu, Xiaoyang Wang, Hongming Zhang, Zongxia Li, Ruosen Li, Jiaxin Huang, Haitao Mi, Dong Yu<br>
  <a href="https://arxiv.org/abs/2508.05004">[Preprint]</a><a href="https://github.com/Chengsong-Huang/R-Zero">[code]</a></p>

<p>VisPlay: Self-Evolving Vision-Language Models from Images<br>
  Yicheng He*, <ins>Chengsong Huang*</ins>, Zongxia Li*, Jiaxin Huang, Yonghui Yang<br>
  <a href="https://arxiv.org/abs/2511.15661">[Preprint]</a><a href="https://github.com/bruno686/VisPlay">[code]</a></p>


<p>Guided Self-Evolving LLMs with Minimal Human Supervision<br>
  Wenhao Yu, Zhenwen Liang, <ins>Chengsong Huang</ins>, Kishan Panaganti, Tianqing Fang, Haitao Mi, Dong Yu<br>
  <a href="https://arxiv.org/abs/2512.02472">[Preprint]</a></p>


<p>Efficient Test-Time Scaling via Self-Calibration<br>
  <ins>Chengsong Huang</ins>, Langlin Huang, Jixuan Leng, Jiacheng Liu, Jiaxin Huang<br>
  <a href="https://arxiv.org/abs/2503.00031">[Preprint]</a></p>


<!-- <p>Divide, Reweigh, and Conquer: a Logit Arithmetic Approach for In-Context Learning<br>
<ins>Chengsong Huang</ins>, Langlin Huang, Jiaxin Huang<br>
<a href="https://arxiv.org/pdf/2410.10074.pdf">[Preprint]</a></p> -->

<h3 id="publications">Publications</h3>

<p> GOFA: A Generative One-For-All Model for Joint Graph Language Modeling<br>
  Lecheng Kong, Jiarui Feng, Hao Liu, <ins>Chengsong Huang</ins>, Jiaxin Huang, Yixin Chen, Muhan Zhang<br>
  <em> In Proceedings of ICLR 2025</em><br>
  <a href="https://arxiv.org/pdf/2410.09724.pdf">[paper]</a></p>

<p> Taming Overconfidence in LLMS: Reward Calibration in RLHF<br>
  Jixuan Leng, <ins>Chengsong Huang</ins>, Banghua Zhu, Jiaxin Huang<br>
  <em> In Proceedings of ICLR 2025</em><br>
  <a href="https://arxiv.org/pdf/2410.09724.pdf">[paper]</a></p>

<p>LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition<br>
  <ins>Chengsong Huang*</ins>, Qian Liu*, Bill Yuchen Lin*, Tianyu Pang, Chao Du, Min Lin<br>
  <em> In Proceedings of COLM 2024</em><br>
<a href="https://arxiv.org/abs/2307.13269">[paper]</a></p>


<p>Watermarking PLMs on Classification Tasks by Combining Contrastive Learning with Weight Perturbation<br>
Chenxi Gu, Xiaoqing Zheng, Jianhan Xu, Muling Wu, Cenyuan Zhang, <ins>Chengsong Huang</ins>, Hua Cai, Xuanjing Huang<br>
<em> In Findings of EMNLP 2023</em><br>
<a href="https://aclanthology.org/2023.findings-emnlp.239.pdf">[Paper]</a></p>

<p>TableVLM: Multi-modal Pre-training for Table Structure Recognition<br>
LeiYuan Chen, <ins>Chengsong Huang</ins>, Xiaoqing Zheng, Jinshu Lin, Xuanjing Huang<br>
<em>In Proceedings of ACL 2023</em><br>
<a href="https://www.aclanthology.org/2023.acl-long.137.pdf">[Paper]</a></p>

<p>On Grounded Planning for Embodied Tasks with Language Models<br>
Bill Yuchen Lin*, <ins>Chengsong Huang*</ins>, Qian Liu, Wenda Gu, Sam Sommerer, Xiang Ren<br>
<em>In Proceedings of AAAI 2023</a>.</em><br>
<a href="https://arxiv.org/abs/2209.00465">[Paper]</a>

<p>WebKE: Knowledge Extraction from Semi-structured Web with Pre-trained Markup Language Model<br>
Chenhao Xie, Wenhao Huang, Jiaqing Liang, <ins>Chengsong Huang</ins>, Yanghua Xiao<br>
<em>In Proceedings of CIKM 2021</a></em><br>
<a href="http://dl.acm.org/citation.cfm?id=3482491">[Paper]</a>

<p>Revisiting the Negative Data of Distantly Supervised Relation Extraction<br>
Chenhao Xie, Jiaqing Liang, Jingping Liu, <ins>Chengsong Huang</ins>, Wenhao Huang, Yanghua Xiao<br>
<em>In Proceedings of ACL 2021</a></em><br>
<a href="https://www.aclanthology.org/2021.acl-long.277.pdf">[Paper]</a>


<h3 id="service">Academic Service</h3>
Conference Reviewer: ARR from 2024 to now, AAAI 2026, ICLR 2025, COLM from 2024 to now

<h3 id="internships">Internships</h3>
<p>Jun 2025 - Aug 2025, Research Intern,<br>
  @ Tencent Seattle AI Lab, advised by <a href="https://wyu97.github.io/">Wenhao Yu</a>.</p>


<p>Jan 2023 - Jun 2023, Research Intern,<br>
@ SAIL, Sea AI Lab, advised by <a href="https://siviltaram.github.io/">Qian Liu</a>.</p>

<h3 id="miscellaneous">Miscellaneous</h3>

I played table tennis and football. I also enjoy playing Dota 2, Texas Hold'em, mahjong, and other board games.<br>
I hope to connect with fellow students in St. Louis who share the same interest
  </section>
      <footer>
        
        <p><small>Hosted on GitHub Pages â€” Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="/assets/js/scale.fix.js"></script>
    
    <script async="" src="https://www.googletagmanager.com/gtag/js?id=G-3HCPXF7QM7"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-3HCPXF7QM7');
    </script>
</body>
</html>
